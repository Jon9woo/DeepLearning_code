{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'these', 'days', 'word', 'em', '##bed', '##ding', '##s', 'are', 'important', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# WordPiece tokenization이 어떤식으로 진행되는지 확인해 보기\n",
    "text = \"These days word embeddings are important.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "these         2,122\n",
      "days          2,420\n",
      "word          2,773\n",
      "em            7,861\n",
      "##bed         8,270\n",
      "##ding        4,667\n",
      "##s           2,015\n",
      "are           2,024\n",
      "important     2,590\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for token, index in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(token, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'was', 'he', 'is', 'as', 'for', 'on', 'with', 'that', 'it']\n"
     ]
    }
   ],
   "source": [
    "print(list(tokenizer.vocab.keys())[2000:2010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
    "       \"fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'after', 'stealing', 'money', 'from', 'the', 'bank', 'vault', ',', 'the', 'bank', 'robber', 'was', 'seen', 'fishing', 'on', 'the', 'mississippi', 'river', 'bank', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for token, index in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(token, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(tokens_tensor, segments_tensors)\n",
    "# See https://huggingface.co/transformers/model_doc/bert.html#bertmodel to know what results have been returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[0].shape # Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[-1].shape # 12th encoder block (the last one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3227e-01, -2.7622e-01, -3.4954e-01,  4.0302e-01,  6.2672e-01,\n",
       "         9.2857e-01,  3.5427e-01,  4.1879e-01,  9.3065e-02, -9.8462e-01,\n",
       "         4.7324e-01, -2.9774e-01, -1.2410e-01,  2.7019e-01,  1.5415e-01,\n",
       "        -2.6653e-01,  3.5623e-01,  4.4153e-01, -1.3475e-01, -5.7552e-01,\n",
       "         1.0842e-01, -7.2393e-01, -4.7583e-01,  3.3128e-01,  1.7051e-01,\n",
       "         7.4757e-02, -1.2539e-01, -4.8409e-01, -3.3479e-01,  3.1896e-01,\n",
       "         1.6756e-01, -1.7411e-01,  1.8103e-01, -4.9209e-01, -5.0967e-01,\n",
       "        -2.2739e-01,  8.4164e-02, -4.9849e-03, -1.1094e-01,  8.3904e-01,\n",
       "        -4.3651e-01,  1.5201e-01, -1.6974e-01,  5.5162e-01,  6.7165e-01,\n",
       "        -4.4596e-01,  2.7928e-01, -1.8881e-01,  2.3252e-01, -5.3890e-01,\n",
       "        -5.1097e-01,  2.4364e-03, -2.9613e-01,  3.1727e-01, -6.3845e-02,\n",
       "         4.9379e-01, -2.2104e-01,  1.0200e-01,  7.0416e-01, -3.9356e-01,\n",
       "        -1.3739e-01,  2.2453e-01,  4.6471e-01, -4.9932e-01,  7.5563e-02,\n",
       "        -5.1134e-02, -6.7853e-02, -2.0015e-01,  2.8201e-01, -3.2341e-02,\n",
       "        -1.7724e-01,  3.0582e-01, -2.4030e-01, -2.1661e-01,  1.9599e-01,\n",
       "         7.6959e-01,  2.1305e-01, -2.6972e-01, -4.6606e-01,  2.0540e-01,\n",
       "         1.4244e-01,  8.4021e-01, -5.7167e-01, -4.2993e-01,  1.9661e-02,\n",
       "         5.9004e-01, -6.2084e-01,  7.2303e-01,  1.0218e-01,  3.9961e-01,\n",
       "        -4.8109e-01,  9.4111e-02, -2.7058e-01,  2.7861e-01, -1.4999e-01,\n",
       "        -5.5724e-01,  4.9837e-02,  1.6448e-01,  5.0656e-01,  9.3804e-02,\n",
       "         2.3358e-02, -8.4397e-01, -2.7041e-01, -1.3912e-01, -4.5874e-02,\n",
       "         1.5588e-02, -2.1654e-01,  3.6170e-01, -8.3540e-02, -1.1149e+00,\n",
       "         1.0556e-01, -1.9060e-01, -1.0755e-01,  6.7990e-02, -4.9355e-01,\n",
       "         7.0187e-01,  9.7763e-01, -5.5027e-02, -5.2621e-01, -3.1034e-01,\n",
       "        -2.1576e-01,  7.5390e-01,  2.2033e-01, -4.2875e-02, -4.0137e-01,\n",
       "        -1.2423e-01,  2.3085e-01, -2.8658e-01,  3.9881e-01, -7.2992e-01,\n",
       "        -3.1923e-01, -7.0877e-02,  1.4260e-01, -2.6479e-01, -2.9423e-01,\n",
       "         3.9750e-01, -7.6349e-02,  7.7096e-02, -6.8957e-01,  2.9310e-01,\n",
       "        -1.3012e-01, -4.2439e-01, -4.5955e-01, -4.2276e-01, -5.1266e-01,\n",
       "         3.3419e-01,  1.1851e-01, -1.1355e-01, -2.4800e-01,  5.1339e-01,\n",
       "        -2.4200e-01, -1.8366e-01, -2.9587e-01, -1.9465e-01,  1.6025e-01,\n",
       "         1.9732e-01, -2.7058e-02,  1.8270e-01,  1.7291e-01, -2.4504e-01,\n",
       "         3.7583e-01,  1.0834e+00, -6.2547e-01, -1.3306e-02, -6.5332e-01,\n",
       "         8.0896e-02,  4.2918e-02, -2.3956e-01,  5.7665e-02,  2.1058e-01,\n",
       "        -4.9129e-01, -1.8178e-01, -7.3936e-02, -1.4541e-01,  3.0363e-01,\n",
       "         3.2088e-01, -3.5534e-01, -1.9670e-01, -1.3694e-02, -4.1983e-01,\n",
       "        -5.6248e-01,  3.8247e-01,  1.8231e-01, -1.8547e-01, -2.7607e-01,\n",
       "        -3.6828e-01, -4.9269e-01, -7.8078e-02, -3.0871e-01,  1.6683e-01,\n",
       "         1.0407e-01, -2.1949e-01, -4.8353e-01, -2.7175e-01,  1.8823e-01,\n",
       "         3.3872e-02, -2.6327e-01, -7.8452e-01, -3.3274e-01, -8.5479e-02,\n",
       "        -7.2709e-01,  2.6681e-01, -1.8506e-01,  2.8090e-02, -6.0512e-02,\n",
       "         8.9798e-01,  4.7527e-02,  6.4847e-02,  3.4279e-01,  4.9297e-01,\n",
       "        -4.7205e-02,  2.3069e-01, -7.4716e-01, -2.1664e-01,  2.1196e-01,\n",
       "        -9.4423e-02, -5.9990e-01,  3.3849e-01,  1.5346e-01, -1.2162e-01,\n",
       "         3.5815e-01,  5.8350e-01, -2.8241e-01,  2.6716e-01,  3.1785e-01,\n",
       "         8.2930e-01, -6.1457e-01,  2.9235e-01, -1.6144e-01, -7.2759e-01,\n",
       "        -1.9249e-01,  5.0150e-02, -3.3251e-02,  4.6680e-01, -7.2297e-01,\n",
       "        -3.7654e-01, -7.1570e-01,  2.3603e-01,  3.4788e-01, -3.9812e-01,\n",
       "         1.1862e-01,  2.4249e-01, -9.5871e-02,  6.4409e-01, -2.0252e-01,\n",
       "        -3.4999e-04,  5.6071e-01,  3.0388e-01, -7.0762e-01,  1.4079e-01,\n",
       "        -3.9458e-01, -4.0896e-01, -4.1461e-01, -6.0947e-01,  3.9385e-01,\n",
       "        -4.5991e-01,  3.9489e-02, -1.6449e-02, -3.4002e-01,  3.5434e-01,\n",
       "         2.5399e-01, -9.2878e-02,  6.0070e-01, -2.7071e-01, -1.4981e-01,\n",
       "        -3.8606e-01,  9.4456e-02, -6.1630e-02,  1.4304e-01,  9.8162e-02,\n",
       "         1.2091e-01, -1.4158e-03,  1.6406e-01, -9.4656e-01, -1.6345e-02,\n",
       "        -1.3201e-01, -8.7546e-02, -3.9558e-02, -7.5118e-02,  2.0157e-01,\n",
       "         1.3555e-01,  3.9707e-02,  2.3160e-01,  1.7413e-01,  6.4509e-01,\n",
       "        -1.1149e+00, -1.3221e-01, -2.3143e-01, -6.1354e-02, -1.8984e-01,\n",
       "        -6.1533e-01,  3.4915e-01, -4.4003e-01,  3.8487e-01, -2.1911e-01,\n",
       "        -4.5310e-01,  1.2044e-01,  2.6860e-01,  7.8542e-02,  7.0868e-02,\n",
       "         1.1399e-01,  3.4535e-01, -6.5707e-01,  6.3502e-01, -2.3034e-01,\n",
       "        -1.3134e-01,  1.7537e-02,  7.1606e-01, -5.4231e+00,  4.2370e-01,\n",
       "         4.0904e-01,  3.1520e-01,  1.2985e-01, -7.1250e-01,  9.4254e-01,\n",
       "        -2.7271e-01, -4.0016e-01,  7.6629e-01, -5.9051e-02, -4.1446e-01,\n",
       "         5.0889e-02,  1.3805e-01, -1.1600e-02, -1.3767e-01,  5.3911e-01,\n",
       "        -4.3391e-01,  3.6389e-01,  6.4302e-01,  6.8861e-01, -6.0045e-02,\n",
       "        -1.1634e-01, -3.0866e-01,  3.2515e-01,  4.7742e-01, -4.5494e-01,\n",
       "        -6.5420e-02,  9.1344e-02,  4.5168e-01,  2.2431e-02, -1.4429e-01,\n",
       "         5.5418e-03,  1.0603e+00, -5.8074e-04,  1.5645e-01, -2.8214e-01,\n",
       "        -5.5468e-02,  1.3590e-01,  9.0823e-02,  6.0598e-02, -2.3505e-01,\n",
       "         1.1219e-01,  5.7742e-01,  1.1431e+00,  3.5992e-01,  5.0611e-01,\n",
       "        -3.4512e-01,  4.5752e-01,  1.7606e-01,  1.1803e-01, -3.7364e-01,\n",
       "         1.1069e-01,  4.0105e-01,  6.7985e-01,  1.0462e-01,  4.0874e-01,\n",
       "         4.6106e-01,  1.2795e-01, -2.9819e-01, -4.5602e-01, -6.2015e-01,\n",
       "         2.2217e-02,  1.0794e-01,  2.9406e-01,  1.6043e-01,  3.0950e-01,\n",
       "         7.3717e-02,  6.3680e-01, -2.4934e-01,  3.0356e-01,  2.1995e-01,\n",
       "         7.9878e-02, -9.1584e-01, -5.3969e-02,  4.3507e-01,  7.1146e-01,\n",
       "        -1.2380e-01,  1.7338e-01,  1.3382e-01, -9.2378e-01, -4.6519e-01,\n",
       "         3.5158e-01,  7.0299e-02, -3.5339e-01, -3.6211e-01, -1.1591e-01,\n",
       "         7.1037e-02, -2.4610e-01,  4.6328e-01,  2.4195e-01,  9.1920e-01,\n",
       "        -2.8645e-01,  8.5881e-01,  2.9050e-01,  1.4875e-01,  2.2668e-01,\n",
       "        -8.6295e-01, -8.7663e-03, -4.1522e-01, -5.4491e-01, -9.4447e-01,\n",
       "         3.2000e-01,  6.6761e-02,  3.3731e-03, -3.0234e-01, -1.1240e-01,\n",
       "         5.7699e-02,  1.9997e-01,  2.9674e-01, -3.4056e-01, -7.8973e-01,\n",
       "         6.2086e-01,  3.0054e-01,  1.7242e-02,  3.7626e-01,  1.0109e-01,\n",
       "         3.1161e-01, -1.5620e-01,  9.1436e-02,  2.1764e-01,  2.7498e-01,\n",
       "        -9.0015e-01, -4.9793e-01,  3.7094e-01, -5.4498e-03,  2.7540e-01,\n",
       "         1.6489e-01, -3.1284e-01,  1.1770e-01, -2.5687e-01, -4.9594e-01,\n",
       "        -5.4285e-01,  2.8356e-01,  4.6359e-01, -2.4191e-01, -3.6919e-01,\n",
       "        -1.6546e+00,  2.9285e-01,  4.4717e-01, -2.5592e-01, -2.4943e-01,\n",
       "        -2.8549e-01,  1.4408e-01, -2.4922e-01,  1.0968e-01,  4.3893e-01,\n",
       "         6.2477e-02,  3.7038e-01,  4.5657e-01, -2.4603e-01,  1.5386e-01,\n",
       "         2.1302e-01, -4.7052e-01,  1.3781e-01,  1.5456e-01,  3.7679e-01,\n",
       "        -1.1551e-01, -4.7879e-02,  3.0249e-01,  1.4059e-01,  5.3941e-01,\n",
       "         1.3345e-01, -1.8527e-01,  1.6658e-03,  4.1585e-01, -1.2871e-01,\n",
       "         7.0595e-02,  5.0229e-01,  7.5431e-01,  1.5079e-01, -7.3401e-02,\n",
       "         6.1245e-01, -3.2793e-01,  2.6861e-01, -5.4929e-01, -8.5916e-02,\n",
       "         1.2679e-01, -4.0748e-01, -1.2968e-01, -2.8432e-01,  5.0817e-01,\n",
       "        -3.5262e-01,  1.8054e-01,  2.1518e-01,  3.0933e-02, -4.4489e-01,\n",
       "         6.6555e-01,  4.1732e-02,  4.4229e-01,  7.7734e-02,  4.7579e-01,\n",
       "        -1.1626e-01,  2.1259e-01, -3.1117e-01, -1.1469e-01, -4.7603e-01,\n",
       "        -5.3599e-02,  1.2080e-01,  6.3966e-01, -2.7397e-01, -5.4636e-02,\n",
       "        -2.3785e-02, -6.7663e-01,  2.7136e-01, -2.8326e-01,  9.4557e-02,\n",
       "         1.4193e-01, -2.6932e-01,  4.1115e-01, -2.3270e-01,  2.2124e-01,\n",
       "        -7.7241e-04, -5.6678e-01, -4.3931e-01, -3.8999e-01,  1.5482e-01,\n",
       "         5.5441e-01,  5.9504e-02,  2.2106e-01,  3.3988e-01,  3.7172e-01,\n",
       "         2.4183e-01,  5.7855e-02, -2.7688e-01,  2.2099e-01, -3.4428e-02,\n",
       "        -2.4210e-01,  4.6864e-01, -2.2757e-01,  2.5185e-01, -9.2740e-01,\n",
       "        -3.1699e-02,  3.8008e-01,  8.7385e-02, -4.4563e-01,  2.2521e-01,\n",
       "        -7.8802e-02,  2.9638e-01,  2.6387e-01, -1.4531e-02, -2.5676e-01,\n",
       "         4.5078e-01,  4.1002e-01,  2.4067e-01,  3.3899e-01, -2.0361e-01,\n",
       "         3.0102e-01,  1.0913e-01,  1.5861e-01,  4.4030e-01,  4.0020e-01,\n",
       "         5.4158e-01, -1.2480e-01,  1.1122e-01, -5.7902e-01, -6.9956e-01,\n",
       "        -5.3477e-02, -1.5926e-02,  2.4735e-01,  1.5984e-01, -5.9913e-01,\n",
       "        -4.5721e-01,  2.4862e-01,  4.6477e-01, -5.1391e-01,  4.1280e-01,\n",
       "         2.6244e-01, -4.5241e-01, -1.1657e-01, -6.4978e-01, -8.3120e-02,\n",
       "        -1.1438e-01,  1.5252e-02,  7.7106e-02,  4.1162e-01,  6.9119e-01,\n",
       "        -7.1029e-01, -1.0827e+00,  3.8595e-02,  7.4982e-01,  3.3305e-01,\n",
       "         4.2572e-01, -3.7768e-01, -6.6134e-01, -2.5744e-01,  9.4729e-02,\n",
       "         5.0738e-01, -7.2770e-01, -2.5351e-01,  7.5622e-01,  6.5307e-01,\n",
       "         7.9941e-01,  1.2460e-01,  7.1316e-01,  3.9403e-03, -7.9148e-02,\n",
       "         2.6380e-01, -3.5723e-01,  2.9231e-01, -7.1502e-01,  3.3878e-01,\n",
       "        -1.5796e-01,  1.2802e-02, -3.0579e-01, -5.6519e-02,  1.2075e-01,\n",
       "        -1.8731e-01,  6.6882e-01, -2.3448e-01, -7.4425e-02, -3.4890e-02,\n",
       "        -1.7502e-01, -1.8051e-01, -7.2192e-01, -7.1838e-01,  5.3621e-01,\n",
       "        -6.9501e-01, -1.2924e-01,  1.2859e-01,  2.0692e-01, -7.6748e-01,\n",
       "         7.5666e-02,  1.5613e-01,  8.0478e-01,  6.8225e-01, -1.8126e-01,\n",
       "         5.2488e-01, -7.5679e-01, -1.9934e-01,  4.9196e-02, -2.9297e-01,\n",
       "        -7.9727e-02,  4.7135e-01, -5.4674e-01,  3.5856e-02,  1.3393e-01,\n",
       "        -7.8227e-01,  1.2640e-01, -6.3078e-01,  1.0670e+00,  2.5418e-01,\n",
       "         1.8108e-01,  7.0106e-01,  4.2679e-01,  8.0630e-02, -1.0078e-01,\n",
       "         2.4507e-01,  4.1110e-01, -8.2112e-02,  2.0995e-01,  3.5882e-01,\n",
       "         3.7735e-01, -6.7377e-01,  5.3843e-01,  4.6488e-03, -4.2360e-02,\n",
       "        -1.5780e-02, -6.9024e-01, -1.3398e+00,  3.3147e-01, -4.9201e-01,\n",
       "         1.0700e+00, -5.5964e-01,  3.6466e-01, -3.7179e-01, -2.3822e-01,\n",
       "        -1.5665e-01, -1.2773e-01,  4.2487e-01,  2.9824e-01,  3.3036e-01,\n",
       "         1.5614e-02, -4.6435e-02, -1.3397e-02,  5.6878e-01,  1.3537e-01,\n",
       "        -4.3541e-01,  4.0425e-02, -6.7405e-02,  2.8802e-01,  2.2107e-01,\n",
       "        -4.5465e-01, -1.1418e+00, -8.0797e-02, -4.0090e-01, -2.6464e-01,\n",
       "        -4.8588e-01, -8.6392e-02, -2.4566e-01,  1.2515e-01, -6.7385e-02,\n",
       "        -6.6673e-03, -5.8031e-01, -1.3743e-01, -2.4133e-01,  3.3213e-01,\n",
       "         4.1755e-01,  4.2034e-01, -2.6394e-01,  6.5976e-01,  2.8644e-01,\n",
       "        -2.9926e-02, -2.1579e-01, -7.1735e-01,  5.5552e-01, -6.0311e-01,\n",
       "         1.1980e-01, -1.1746e+00,  7.1608e-02,  7.0845e-01, -3.3557e-01,\n",
       "        -4.2835e-01,  1.2524e-01,  4.2594e-01,  1.1123e-02,  4.2534e-01,\n",
       "        -2.0217e-01, -4.0462e-01, -7.6270e-02,  7.6203e-02,  4.6099e-01,\n",
       "         1.3967e-01,  6.5740e-01,  1.7061e-01, -5.8953e-01, -1.0800e-02,\n",
       "         7.9500e-02,  1.4121e-01,  7.5959e-02, -2.5899e-02, -2.9216e-01,\n",
       "         1.0510e-01, -1.4112e-01,  4.2280e-01, -7.1700e-01, -2.4229e-01,\n",
       "         8.3139e-01, -5.7732e-01,  5.4283e-01,  2.2209e-01,  8.5735e-02,\n",
       "         2.9427e-01, -4.7248e-01, -2.2468e+00, -2.2912e-01, -1.0453e+00,\n",
       "        -2.3270e-02,  4.8659e-01, -1.2218e-01,  3.0739e-01, -8.2270e-01,\n",
       "         4.0395e-01,  2.1360e-01,  5.2498e-01,  2.2878e-01,  2.6951e-01,\n",
       "        -4.5666e-01,  3.7865e-01, -1.0961e-01], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[-1][0][1] # 마지막 encoder block에서 출력하는 'after' 토큰에 대한 hidden state 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 22, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_encoder_outputs = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "total_encoder_outputs .size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 22, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "total_encoder_outputs = torch.squeeze(total_encoder_outputs, dim=1)\n",
    "\n",
    "total_encoder_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 13, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "total_encoder_outputs = total_encoder_outputs.permute(1,0,2)\n",
    "\n",
    "total_encoder_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7489e-01, -2.5283e-01, -1.5946e-01,  2.2309e-02,  1.3923e-01,\n",
       "         1.0140e-01, -1.7607e-01,  3.6460e-01, -2.9223e-01, -3.4522e-01,\n",
       "        -1.1963e-01,  7.3653e-02, -6.8991e-02,  4.0090e-02, -2.6117e-01,\n",
       "         2.3941e-01, -5.0902e-02,  4.7517e-02,  3.2660e-01,  1.2581e-01,\n",
       "         1.4941e-01,  1.3089e-01,  1.2608e-01,  1.4950e-01,  1.0367e-01,\n",
       "         1.1399e-01, -3.4984e-02,  1.4137e-02, -4.6549e-01,  1.2883e-01,\n",
       "         9.0494e-02, -3.2566e-01,  2.6344e-01, -3.4416e-01, -1.8339e-01,\n",
       "        -4.8529e-02, -1.2271e-01,  5.2017e-02, -2.1477e-01,  7.3059e-02,\n",
       "         7.7150e-02,  8.1869e-02, -2.9393e-01, -1.7131e-01,  2.2285e-01,\n",
       "        -2.1327e-01,  1.0837e-01,  9.2270e-02,  1.4877e-01, -9.0220e-02,\n",
       "         2.4494e-01, -1.1656e-01, -1.7681e-01, -6.9747e-01, -2.7377e-02,\n",
       "        -3.7989e-01,  3.8025e-01,  1.6087e-01,  2.8276e-01, -1.3439e-01,\n",
       "         5.0344e-02, -3.3081e-02,  1.4906e-01, -2.6149e-01,  1.0969e-02,\n",
       "        -5.0950e-02, -5.7089e-02, -7.5764e-02,  5.2206e-01, -9.2873e-02,\n",
       "         1.9443e-01,  1.1372e-01, -1.6781e-01, -1.9136e-01, -1.6792e-01,\n",
       "        -5.4146e-01,  6.8595e-02,  6.2544e-02, -2.5036e-01,  9.7374e-02,\n",
       "        -3.5865e-01, -2.0106e-01, -1.6131e-01,  2.9147e-01,  2.4419e-01,\n",
       "        -4.5345e-02, -5.5346e-02,  1.7911e-01,  2.2443e-01, -1.7316e-01,\n",
       "         6.2756e-02,  2.2305e-01, -8.4514e-01, -2.4097e-01,  1.3283e-02,\n",
       "         1.3808e-01, -1.3442e-01, -2.1330e-01, -1.0113e-01, -1.7870e-01,\n",
       "        -4.4457e-02, -1.0130e-01,  1.5210e-01,  9.1004e-02,  1.1014e-02,\n",
       "         1.2251e-01,  7.9043e-02, -3.5709e-01, -2.2153e-01,  5.6533e-01,\n",
       "         7.8538e-02,  2.7339e-01,  7.2934e-02,  2.4158e-02,  2.9761e-03,\n",
       "         2.1632e-01, -3.7454e-01, -6.4005e-02,  3.2652e-01,  2.6391e-01,\n",
       "         4.2788e-01,  1.0874e-01, -6.2790e-02,  2.4309e-01,  3.1182e-01,\n",
       "        -8.5904e-02, -1.6208e-01, -1.6804e-01, -1.0720e-01, -2.8217e-01,\n",
       "         1.9067e-01,  1.0147e-01, -4.5442e-02,  6.0230e-03,  1.7466e-01,\n",
       "         1.9470e-02,  2.0041e-01, -7.7916e-02, -1.8202e-01,  3.0242e-01,\n",
       "         4.2501e-02, -1.1935e-01,  3.6073e-01,  5.8374e-02, -2.3071e-02,\n",
       "        -6.3844e-02,  2.7985e-02,  1.4218e-01,  1.5697e-01,  1.9875e-01,\n",
       "        -2.9569e-01, -2.7544e-02, -1.3925e-01, -2.1671e-02, -2.9877e-01,\n",
       "        -7.4949e-02,  2.0568e-02, -9.6738e-02,  1.2780e-01,  1.5372e-02,\n",
       "         1.0953e-01, -5.0334e-02,  1.7033e-01,  1.4890e-01,  1.6176e-02,\n",
       "         6.0030e-02,  1.9395e-01,  3.5922e-01, -1.9717e-01, -1.2194e-01,\n",
       "         9.9558e-02,  8.7407e-02, -3.4051e-01, -1.5994e-01, -1.3605e-02,\n",
       "        -2.7292e-02,  1.7209e-01,  5.4685e-02,  3.0186e-01,  2.7446e-01,\n",
       "        -4.1807e-02, -1.1192e-01, -4.8269e-03, -1.5364e-01, -9.6184e-02,\n",
       "         1.0188e-01, -2.1160e-01,  1.7044e-01,  3.7449e-02, -1.3604e-01,\n",
       "        -1.3313e-01,  1.0145e-01,  2.9266e-01, -1.8101e-01, -2.1925e-01,\n",
       "         1.0652e-01, -8.8377e-02, -1.9547e-01, -3.7206e-01, -1.1453e-01,\n",
       "         1.8544e-01, -3.7733e-01, -1.6749e-01,  6.6932e-02,  2.0285e-01,\n",
       "         8.0670e-02, -2.8138e-01,  5.1609e-01, -6.5277e-02, -1.7680e-01,\n",
       "         2.5017e-02,  1.3929e-01, -1.3178e-01, -1.3508e-01,  5.7184e-02,\n",
       "         4.4322e-02, -5.0454e-02, -2.5399e-01,  3.4564e-01, -1.2443e-01,\n",
       "        -1.9996e-01,  2.8389e-01, -7.4137e-02, -2.0807e-01,  1.6528e-01,\n",
       "        -2.2457e-01,  3.5054e-01,  5.1778e-01, -1.5445e-01,  5.0168e-02,\n",
       "         1.7968e-01, -1.3601e-01,  2.4876e-01, -1.6944e-02, -3.0223e-02,\n",
       "         2.5890e-01, -1.4621e-01, -3.9137e-01,  2.3704e-02,  2.8355e-01,\n",
       "        -8.1472e-02,  1.1542e-01,  3.0797e-01,  7.8527e-02, -1.6218e-02,\n",
       "        -5.2988e-01,  9.4665e-02, -2.1921e-01, -7.3447e-02, -4.8347e-02,\n",
       "        -4.8673e-01, -7.7046e-02,  2.7254e-03,  1.0671e-01,  6.2756e-02,\n",
       "        -7.4207e-02, -2.3519e-01, -1.2676e-01,  1.8161e-01, -4.5883e-01,\n",
       "        -2.0532e-01, -1.1107e-02,  2.1536e-01,  2.7072e-01, -3.3059e-02,\n",
       "        -8.4776e-02, -4.9486e-02, -1.1380e-01, -7.0787e-02, -2.5579e-01,\n",
       "         4.0784e-01,  1.0653e-01, -4.7578e-02, -1.3563e-03, -1.2017e-01,\n",
       "         1.1490e-02,  2.1538e-02,  2.7563e-01,  4.6889e-01, -2.7201e-01,\n",
       "        -2.0562e-01,  2.4977e-03, -8.8444e-02, -1.0606e-02,  8.6075e-02,\n",
       "         5.9546e-01,  2.1604e-01, -4.0333e-01,  1.1897e-01,  2.2309e-01,\n",
       "        -4.3030e-01, -4.9067e-02, -2.6187e-01,  7.0690e-02,  8.6125e-02,\n",
       "        -2.7470e-01,  7.9988e-01,  2.5502e-02, -2.2616e-01, -1.0531e-01,\n",
       "        -7.9775e-04, -2.1345e-01, -4.6896e-02, -1.1183e-01,  2.9831e-03,\n",
       "        -1.1470e-01, -2.5434e-02, -7.5480e-02,  3.9707e-02, -8.3212e-02,\n",
       "        -3.1875e-02,  4.6842e-01,  1.3604e-01,  1.9081e-01,  1.9023e-02,\n",
       "        -2.4577e-02, -1.6283e-02, -4.0947e-02,  2.8361e-01, -1.3282e+00,\n",
       "        -2.4707e-02, -4.8589e-02, -6.4461e-02,  4.2332e-01, -1.4192e-01,\n",
       "         1.8360e-01, -7.1383e-04, -3.9538e-02, -2.3668e-01,  1.7705e-01,\n",
       "         2.4784e-01,  2.0175e-01,  2.4271e-04, -1.3973e-01,  2.8671e-01,\n",
       "         3.6045e-01, -1.8258e-01,  2.5583e-01, -2.2745e-01, -7.4686e-02,\n",
       "         2.8141e-02,  1.6991e-01,  2.6239e-01,  1.5678e-01,  2.7571e-02,\n",
       "        -6.0292e-02,  5.6896e-02,  3.2282e-01,  8.4193e-02,  1.4289e-01,\n",
       "         2.7346e-01, -2.5472e-01,  3.8508e-01,  4.7626e-02, -9.3558e-02,\n",
       "        -4.2112e-02, -6.1671e-02,  5.3811e-01, -9.0861e-02, -4.0036e-01,\n",
       "         2.2595e-01,  4.8532e-01, -1.2157e-01,  4.3028e-02, -1.1561e-01,\n",
       "        -3.5776e-01,  3.9375e-01,  4.0619e-01, -1.8479e-01, -1.3382e-01,\n",
       "         8.8752e-02,  1.7569e-01, -1.2995e-02,  7.8177e-02, -8.6701e-02,\n",
       "        -5.4690e-02,  2.4901e-01,  4.1644e-01, -2.4518e-01,  4.7893e-02,\n",
       "         7.3697e-02,  6.7075e-02,  1.4415e-01, -1.4403e-02, -8.1347e-02,\n",
       "         1.0755e-01, -2.2629e-01,  9.9272e-02,  3.8890e-01,  3.4929e-01,\n",
       "         4.7752e-01, -2.3441e-01,  2.0891e-01, -8.3826e-02, -6.4029e-02,\n",
       "         7.8407e-02, -1.8658e-01, -3.6616e-01,  1.6816e-01,  6.4108e-02,\n",
       "         3.4606e-01,  3.0428e-01, -1.3627e-01, -7.2003e-02,  7.9126e-02,\n",
       "        -2.4555e-01,  4.8725e-02,  1.3806e-01,  1.6737e-02,  2.4032e-01,\n",
       "         2.5790e-01,  1.4257e-01, -1.2766e-01,  2.9538e-01, -5.4378e-01,\n",
       "        -4.4756e-01,  1.4486e-01,  1.1810e-01,  3.5331e-01,  2.5405e-01,\n",
       "        -1.2075e-01,  4.5438e-02,  1.0154e-01, -2.0333e-01,  3.6784e-02,\n",
       "        -5.4696e-02,  1.0875e-01,  5.8824e-02, -3.5229e-01, -3.7248e-01,\n",
       "        -1.5483e-02,  3.1977e-01,  7.4363e-02, -1.5210e-01,  3.3583e-02,\n",
       "        -2.2551e-01,  1.5799e-02,  7.0612e-02,  1.3006e-01, -3.1346e-01,\n",
       "         2.4586e-01, -1.9129e-01, -6.1177e-01,  1.1307e-01,  2.1147e-01,\n",
       "         1.3270e-01,  1.2139e-01, -1.4429e-01,  1.8179e-01, -5.5822e-01,\n",
       "         2.2355e-01,  1.1256e-01, -1.5673e-01,  2.7552e-01,  9.3066e-02,\n",
       "        -1.2632e-01,  1.5133e-01, -8.2485e-02,  4.4158e-01, -3.0161e-02,\n",
       "         1.1422e-02,  4.1356e-01,  4.2922e-02,  1.2266e-01, -8.9177e-02,\n",
       "         6.0920e-01, -3.4836e-04, -5.4160e-01, -2.2033e-01, -2.9458e-01,\n",
       "         4.9370e-02, -3.3271e-01, -2.2400e-03, -4.8220e-02, -5.6902e-01,\n",
       "         5.3147e-02,  1.0458e-01,  3.8622e-01,  4.6697e-02,  5.3194e-02,\n",
       "         1.4189e-01, -1.5289e-01,  4.5478e-01,  1.5525e-01,  3.5634e-02,\n",
       "         1.0025e-01, -2.0211e-01,  6.9956e-02, -1.4347e-01,  9.9425e-02,\n",
       "         3.1992e-01, -5.5700e-02,  2.0221e-01,  2.8406e-02,  1.5227e-01,\n",
       "         3.3088e-01,  1.6177e-01,  1.9672e-01,  1.4308e-01, -3.1045e-01,\n",
       "         3.8504e-01, -2.7121e-01, -4.6743e-01,  6.4087e-04, -9.3301e-02,\n",
       "        -2.8293e-01,  1.3577e-01,  3.0460e-03, -1.5391e-01,  4.2121e-02,\n",
       "         2.8006e-01,  6.4647e-03, -6.8069e-02, -1.1512e-01, -2.9755e-01,\n",
       "        -1.5755e-02,  2.6509e-02, -7.6963e-02,  1.9089e-01, -1.2638e-01,\n",
       "         1.5971e-01,  3.6719e-01, -5.8390e-02, -1.5692e-01, -8.4946e-02,\n",
       "        -4.6411e-02, -4.0633e-01,  1.1026e-01, -1.8690e-01, -1.2925e-02,\n",
       "        -5.0751e-01, -2.2561e-01, -1.6930e-02,  8.0423e-02,  2.2518e-02,\n",
       "        -4.2836e-02,  1.5629e-02,  1.7016e-01, -5.8492e-02, -2.0230e-01,\n",
       "        -1.4664e-01, -5.2073e-03, -2.3908e-02, -1.1509e-01, -2.4091e-01,\n",
       "         3.9739e-02, -3.9304e-01,  3.0114e-01, -6.6929e-02, -8.3947e-02,\n",
       "        -4.3422e-02,  2.6405e-01,  1.2001e-01,  2.1086e-01, -2.7903e-02,\n",
       "        -3.4595e-02, -7.3750e-01, -2.0017e-01,  3.0520e-01,  9.3576e-02,\n",
       "         7.2827e-02, -1.9576e-01, -6.0032e-02, -5.7209e-02,  3.9954e-01,\n",
       "         1.4649e-01,  3.4084e-01, -1.6383e-01, -1.9839e-01,  3.3088e-02,\n",
       "        -8.8608e-02,  3.9941e-01,  2.6638e-01,  8.2702e-02,  1.6294e-01,\n",
       "        -8.2837e-02,  2.5891e-01,  4.6475e-01,  7.4987e-02, -2.1527e-01,\n",
       "        -1.9711e-01, -2.0594e-01, -1.8560e-01, -6.1787e-02,  8.4946e-02,\n",
       "        -6.5000e-01,  1.8835e-02,  1.2532e-01,  1.0027e-01,  3.5873e-01,\n",
       "        -8.1967e-02,  9.2657e-02, -1.7745e-01,  1.4441e-02,  1.4753e-01,\n",
       "         1.2895e-01, -3.5724e-03, -8.1947e-02,  2.6258e-01, -1.1808e-01,\n",
       "         5.4100e-04, -1.7151e-01, -1.3439e-01, -1.2632e-01,  8.0181e-02,\n",
       "        -3.5363e-01, -2.2561e-01,  3.7163e-01, -4.5365e-02,  1.2743e-01,\n",
       "        -3.1955e-01, -6.0409e-02,  8.6153e-02,  1.6662e-01,  3.6072e-01,\n",
       "         2.3623e-01, -1.6293e-01,  3.0387e-02, -1.1948e-01,  1.5442e-01,\n",
       "        -2.7648e-01, -9.5559e-02,  1.1697e-01,  9.6064e-02, -7.6664e-02,\n",
       "        -4.7169e-01, -1.2880e-01, -2.9183e-01, -2.3002e-01,  1.9005e-01,\n",
       "        -4.4509e-01,  2.4804e-01, -5.0901e-01,  1.2060e-02, -1.6149e-01,\n",
       "         1.7214e-01, -1.2359e-01, -7.1408e-02,  3.7977e-02, -1.4238e-01,\n",
       "        -1.9495e-02,  1.5617e-01,  9.6965e-03, -1.1738e-01, -1.6065e-01,\n",
       "        -3.3185e-01,  3.3595e-01, -1.8775e-01,  5.0144e-01,  2.0477e-01,\n",
       "        -7.8492e-02,  2.1221e-01, -1.2692e-02, -4.5493e-01, -6.9333e-02,\n",
       "        -9.9680e-03, -3.1194e-01, -1.0482e-01, -1.0057e-01,  1.1028e-01,\n",
       "         3.3923e-02,  1.6590e-02,  2.8785e-03, -6.3658e-02, -8.0514e-02,\n",
       "        -1.3017e-01,  8.4929e-02,  4.6018e-02, -3.0299e-01, -2.6265e-01,\n",
       "        -8.0064e-01,  1.0150e-01, -8.6016e-02, -1.1342e-01,  1.0746e-01,\n",
       "         2.0794e-01, -2.5044e-01,  2.5949e-01, -2.2857e-01, -6.1549e-01,\n",
       "        -6.4536e-02,  8.9343e-02,  3.3435e-01, -3.9411e-02,  1.2235e-01,\n",
       "         2.3359e-01, -1.7270e-01, -2.9762e-01, -2.2352e-01,  1.5845e-01,\n",
       "         7.9334e-02,  3.6530e-01, -2.1772e-01,  9.4766e-02,  2.7207e-02,\n",
       "         2.6350e-01,  1.0989e-01,  5.0695e-01, -1.8874e-01,  3.8554e-01,\n",
       "        -2.2927e-01, -2.0310e-01, -1.8466e-02, -8.9425e-02,  1.5412e-01,\n",
       "         1.8655e-01, -4.5891e-01, -1.8391e-01,  1.6650e-01, -1.2421e-01,\n",
       "        -3.2333e-01, -3.0509e-01,  1.2648e-01, -3.3116e-01,  1.7041e-01,\n",
       "        -8.9921e-02, -5.1593e-02, -1.2063e-01,  3.9054e-02,  1.6294e-01,\n",
       "        -1.3617e-01,  3.2221e-02,  4.7245e-02,  3.1286e-01,  1.9587e-01,\n",
       "        -4.6041e-02,  1.2331e-01,  3.5381e-01, -4.3230e-01, -1.1597e-01,\n",
       "         2.7010e-02,  2.2470e-02, -3.3965e-02,  6.1635e-03,  1.1295e-01,\n",
       "         1.6890e-01, -7.8125e-02, -3.8553e-01, -2.8587e-01, -4.3544e-01,\n",
       "        -1.0143e-01, -7.2142e-02,  2.8327e-01, -8.6262e-02,  1.9715e-01,\n",
       "        -1.9652e-02, -2.1428e-01, -4.7945e-02, -3.4989e-02, -1.2859e-01,\n",
       "         1.1132e-01,  1.5042e-01,  3.6030e-01,  2.1400e-01, -3.1888e-01,\n",
       "         5.3505e-02, -3.2101e-02, -8.1276e-02,  3.1255e-01,  7.4139e-03,\n",
       "        -1.1589e-02, -2.9452e-01,  4.2669e-01,  2.2510e-02,  3.0834e-01,\n",
       "        -3.5400e-01, -2.8731e-02, -1.4710e-01], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_encoder_outputs[6][0]-total_encoder_outputs[10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_encoder_outputs[1][-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_last4_vector = torch.cat((total_encoder_outputs[1][-4], total_encoder_outputs[1][-3], total_encoder_outputs[1][-2], total_encoder_outputs[1][-1]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(after_last4_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_sum_last4_vector = torch.sum(total_encoder_outputs[1][-4:], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(after_sum_last4_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_mean_last4_vector  = torch.mean(total_encoder_outputs.permute(1,0,2)[11], dim=0) # 평균값 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(after_mean_last4_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_vector = total_encoder_outputs[0][-1] #첫번째 토큰, 즉 [CLS]에 대한 마지막 encoder의 hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cls_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank1_vector = torch.sum(total_encoder_outputs[6][-4:], dim=0)\n",
    "bank2_vector = torch.sum(total_encoder_outputs[10][-4:], dim=0)\n",
    "bank3_vector = torch.sum(total_encoder_outputs[19][-4:], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bank1_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank1_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.3596e+00, -2.9805e+00, -1.5421e+00,  7.0651e-01,  2.0031e+00,\n",
       "         6.3182e-01, -2.9078e+00,  1.6307e+00, -1.0581e+00, -2.4467e+00,\n",
       "         1.5198e-01, -1.8649e+00, -8.6627e-01,  1.5591e+00, -4.4090e+00,\n",
       "         1.9979e-01, -3.9120e-01,  4.6780e+00,  4.0063e+00,  1.9281e+00,\n",
       "        -2.6839e+00, -4.5731e-01,  2.9573e+00,  2.0648e+00,  7.1509e-01,\n",
       "         5.0527e-01,  2.9728e+00,  2.0761e+00,  3.0218e-01,  5.9447e-01,\n",
       "         3.2789e+00,  1.3924e+00,  1.7653e+00,  3.6563e-01,  4.8986e-01,\n",
       "        -5.8335e-01, -2.1327e+00,  1.0474e+00, -2.1078e+00,  1.8824e+00,\n",
       "        -5.2616e-01, -1.4409e+00, -1.2064e-02,  1.6986e+00, -1.1851e-01,\n",
       "         1.4369e+00, -1.5800e+00,  2.0784e-01, -4.0648e+00, -2.7408e-01,\n",
       "        -1.2670e+00,  2.7449e+00, -5.6628e+00, -3.8262e+00, -4.5445e-01,\n",
       "         2.0363e+00, -3.3607e+00, -3.4739e+00,  4.6744e-01, -1.8318e+00,\n",
       "         2.7669e+00,  1.1167e+00,  3.0754e+00, -4.9868e+00, -1.8789e+00,\n",
       "        -3.2385e+00,  2.3378e+00,  7.9255e-01, -1.7020e+00,  2.1680e+00,\n",
       "         7.5560e-01,  6.9838e+00, -3.9615e+00, -2.0989e+00,  7.5948e-01,\n",
       "         1.3349e+00, -1.2244e+00,  1.3776e+00,  8.4025e-01,  5.6896e-02,\n",
       "         5.2001e-01, -3.1296e+00, -1.0463e+00,  8.5425e-02,  1.7991e+00,\n",
       "         2.1892e-01,  1.8786e+00,  5.0545e-01,  9.6316e-01,  1.0521e+00,\n",
       "        -2.4139e+00,  2.7457e+00, -8.5468e-01,  4.6217e-01, -2.4779e+00,\n",
       "         1.2351e+00, -7.3478e-01,  3.4264e-01, -1.8389e+00, -1.2183e-01,\n",
       "         1.3439e+00, -7.4503e+00, -2.8162e-01,  2.0328e+00, -7.6248e-01,\n",
       "        -2.6143e-01, -2.8315e+00, -9.2324e-01, -2.2144e+00, -1.4063e-01,\n",
       "         3.6577e+00, -1.8200e+00,  3.0024e-01, -3.3771e+00,  1.4146e+00,\n",
       "        -2.0793e+00,  2.7625e+00, -4.9980e+00, -3.4467e-01, -4.3340e+00,\n",
       "         3.0364e-01,  3.2830e+00,  1.7012e+00,  2.0102e+00,  1.3735e+00,\n",
       "         1.2425e+00,  6.7272e-01,  1.9948e+00,  4.4212e-01,  4.6525e-01,\n",
       "         2.3398e+00,  6.5738e-01, -2.6376e+00, -4.7761e-01,  3.5633e+00,\n",
       "         1.1765e+00, -1.3801e+00, -1.9487e+00, -3.3014e+00, -1.4979e+00,\n",
       "         7.5654e-01,  1.5316e+00, -1.1649e+00,  7.3946e+00,  1.2596e+00,\n",
       "         1.3860e+00, -2.1226e+00, -1.8046e+00, -1.0779e+00,  2.5370e+00,\n",
       "        -1.3095e+00,  2.0166e+00, -1.6355e-01, -5.9248e-01,  3.6053e+00,\n",
       "        -9.8710e-01, -4.6666e+00,  1.2268e+00, -2.4689e+00, -6.2246e-01,\n",
       "         1.5318e+00, -7.8071e-02,  2.1727e+00,  1.3473e+00, -5.7889e+00,\n",
       "        -4.6291e+00, -2.3654e-01,  1.3386e+00, -2.2505e+00, -1.8193e+00,\n",
       "        -2.1977e-01, -1.1154e+00,  1.3061e+00,  8.0055e-01,  3.7304e+00,\n",
       "         2.2318e+00, -1.0265e+00, -2.7786e+00, -6.0441e-02, -2.6188e+00,\n",
       "        -4.5928e-01,  3.2903e+00,  2.5759e+00,  4.1714e+00,  2.7710e+00,\n",
       "        -1.4771e+00,  9.1577e+00,  1.4919e+00,  2.2133e+00, -1.4821e-01,\n",
       "        -1.0689e+00, -3.9213e+00, -2.6481e-01,  4.0377e-01, -2.6790e+00,\n",
       "        -3.2562e-01, -1.9445e+00, -2.0796e+00, -2.5534e+00, -4.2400e-01,\n",
       "        -2.5242e+00, -9.2498e-01, -2.5726e+00, -1.3503e+00, -3.2184e+00,\n",
       "         1.3275e+00, -1.3705e+00,  1.7935e+00,  1.6299e+00, -4.0623e-01,\n",
       "        -4.5599e-01, -1.0608e+00,  7.0237e-01,  2.6678e+00, -7.9520e-01,\n",
       "         1.6379e+00, -1.3666e+00, -4.5793e+00, -7.7615e-01, -1.3190e+00,\n",
       "        -1.3373e+00, -9.4873e-01,  1.2540e+00,  3.3316e-01, -2.6964e+00,\n",
       "         2.5759e+00,  3.5764e+00, -1.0657e+00, -2.8529e+00,  6.1242e-01,\n",
       "        -6.8167e-01,  4.1337e+00,  2.6178e+00,  5.0338e-02,  2.1306e+00,\n",
       "        -1.7019e+00, -6.2588e+00,  1.3904e+00,  1.0263e+00,  2.7240e+00,\n",
       "        -1.7021e+00,  2.4642e-01,  2.4905e+00, -1.0830e-01,  2.0890e-01,\n",
       "         2.5848e+00, -3.3372e-01, -2.6410e+00, -3.8824e+00,  4.6817e-01,\n",
       "        -1.2940e+00, -1.0359e+00, -3.1990e+00, -9.1003e-01,  4.8628e+00,\n",
       "        -5.3192e-01, -6.9404e-01, -3.8708e-01, -2.4679e+00,  1.1418e+00,\n",
       "         2.7012e+00,  2.0498e+00, -2.4234e+00,  9.8582e-01, -8.9772e-01,\n",
       "        -2.3226e+00,  1.2868e+00,  7.5272e-01,  6.8487e-01,  2.3821e+00,\n",
       "         6.6494e-01,  2.6370e+00, -1.7290e-01,  4.4338e+00,  3.5470e+00,\n",
       "        -2.3368e+00, -1.7792e+00, -1.3511e+00,  3.2028e+00,  2.3125e+00,\n",
       "         1.5985e-02,  1.7798e+00,  7.0171e-01,  2.8561e-01,  4.5631e-02,\n",
       "        -1.0385e+00, -4.3744e-01,  5.9990e+00, -1.0119e+00, -2.0311e+00,\n",
       "         5.8261e-01,  7.2692e-02, -1.6293e+00,  8.2998e-01,  6.3420e-01,\n",
       "         8.0359e-01,  1.3805e+00,  5.1296e+00,  2.9098e+00, -4.5344e-01,\n",
       "         2.9878e+00,  3.7041e-01, -9.3406e-01, -4.2934e+00, -1.4559e+00,\n",
       "         1.0637e+00, -1.1091e-01, -3.5207e-01, -2.1603e+01,  5.8433e+00,\n",
       "        -4.0180e-01, -2.2625e+00,  1.1632e+00, -9.1818e-01,  1.0808e+00,\n",
       "        -1.7748e+00, -5.6317e+00,  7.6229e-01, -4.3721e+00, -2.3459e+00,\n",
       "         1.8652e+00, -7.3791e-01, -3.4164e-01, -4.1641e+00,  3.7988e-02,\n",
       "         1.6570e+00, -2.8905e-01, -1.2750e+00, -1.4524e+00, -2.6571e+00,\n",
       "        -4.0869e-01,  1.9733e+00,  2.4995e+00,  2.3789e+00, -2.1928e+00,\n",
       "        -1.0206e+00, -2.9820e-01,  1.6352e+00, -1.7011e+00,  1.9806e-01,\n",
       "        -1.2186e-01,  1.1333e+00, -1.1721e+00, -3.1014e+00, -2.5797e+00,\n",
       "        -2.0401e+00,  2.0368e+00, -3.5684e+00,  6.7430e-01,  3.2502e+00,\n",
       "        -2.6617e+00,  2.2080e+00,  3.2612e+00, -1.5200e+00, -3.5751e-01,\n",
       "         1.9536e+00, -6.9127e-01,  2.8204e+00,  5.8803e-01, -2.0547e+00,\n",
       "         1.1613e+00, -7.2180e-01,  1.2896e+00,  2.0199e+00, -9.2755e-01,\n",
       "         1.8500e+00,  3.5147e+00, -1.1749e+00, -1.3967e+00,  2.0780e+00,\n",
       "         6.3412e-01,  8.5973e-01, -1.3472e+00,  2.8866e-01, -6.0876e+00,\n",
       "        -2.4652e+00, -3.1404e-01, -2.6390e+00, -2.4132e+00,  9.5678e-01,\n",
       "         6.5299e-02, -1.0574e+01, -8.2720e-01, -1.4197e+00,  3.9598e+00,\n",
       "        -3.7142e+00, -4.8080e+00,  9.7540e-01,  3.1145e+00,  2.0072e-01,\n",
       "         2.0583e+00, -4.3325e-01,  1.6166e+00,  1.1740e+00, -1.7391e+00,\n",
       "         1.8823e+00, -5.7051e+00,  3.1488e-01,  2.4112e+00,  6.1922e+00,\n",
       "         2.1473e+00, -2.8707e+00,  5.0811e-01,  2.4893e+00, -1.2533e+00,\n",
       "        -1.1752e+00,  1.5037e+00,  4.9766e-01, -6.9594e-02,  4.0716e+00,\n",
       "         2.7841e+00, -3.0840e-01, -2.6297e+00, -2.7344e+00,  5.6643e-01,\n",
       "        -1.0393e+00, -4.3339e+00,  5.8771e+00,  2.2513e+00, -2.7807e+00,\n",
       "         3.9732e+00, -2.5449e+00, -1.9830e-01,  1.1648e+00, -4.1403e-01,\n",
       "        -3.3741e+00, -1.9379e-01, -2.0152e+00, -4.5673e+00,  5.2855e-01,\n",
       "        -1.6301e+00, -6.2260e+00, -5.0134e-01,  1.6845e+00,  1.3916e+00,\n",
       "        -1.2453e+00, -1.5229e+00, -8.9464e-01,  3.0994e+00, -7.4684e-01,\n",
       "        -2.7875e+00,  4.4515e-01,  1.0711e+00, -3.4941e-01,  3.8073e+00,\n",
       "        -6.6101e-01, -2.0255e+00,  2.4020e-02,  1.3180e+00,  7.8063e-01,\n",
       "         8.5755e-01, -1.2442e+00, -1.0559e-01, -3.3662e+00, -1.4259e+00,\n",
       "         1.3661e+00,  8.7748e-01, -1.0711e+00, -1.2827e+00, -6.5498e-01,\n",
       "         2.3886e+00, -9.3818e-01,  2.4248e+00,  4.3893e+00,  5.2619e+00,\n",
       "        -1.4582e+00, -3.2200e+00, -1.8862e+00,  2.1306e+00, -2.5829e+00,\n",
       "         4.5032e+00,  1.5342e+00, -8.1572e+00, -4.5670e-01, -4.9801e+00,\n",
       "        -2.1089e+00,  5.5342e-01,  1.2704e+00,  1.2727e+00, -2.0687e+00,\n",
       "        -7.3899e-01,  2.5692e+00, -1.3403e+00,  9.5978e-01,  6.7786e-01,\n",
       "        -3.3989e+00, -8.5905e-02,  1.1010e+00, -9.0276e-01,  3.9600e+00,\n",
       "         2.2367e+00, -2.1177e+00,  8.3703e-01,  2.8581e+00,  2.1356e+00,\n",
       "         3.6850e+00, -1.1888e+00,  1.3428e-01, -2.6106e+00,  1.3510e+00,\n",
       "        -6.3359e-01,  9.0754e-01, -3.1145e+00, -3.6928e+00, -7.1253e-01,\n",
       "         1.2737e+00, -4.1464e-01,  3.7917e-01, -4.1644e+00,  1.5985e+00,\n",
       "         8.2823e-03, -2.3963e+00,  5.1524e-01,  1.4219e-02,  1.6385e+00,\n",
       "         9.9086e-01, -2.2635e+00,  3.3663e+00, -4.5370e+00, -1.2580e-01,\n",
       "        -4.6092e+00, -1.9835e+00,  1.3484e+00, -2.4209e+00,  1.0194e+00,\n",
       "        -2.2728e-01, -1.9511e+00, -4.6238e-02,  2.2749e+00,  9.6675e-01,\n",
       "        -9.6656e-01, -1.3634e+00, -2.8797e+00,  6.8233e-01,  1.4712e+00,\n",
       "         1.4553e+00, -1.0151e+00,  1.8099e+00,  2.2618e-02, -5.4728e+00,\n",
       "         7.5378e-01, -6.0859e-01, -1.7737e+00, -4.6772e-01, -8.8314e-01,\n",
       "        -7.1737e+00,  5.8295e-01, -8.5400e-01,  1.5593e-02,  5.4345e-01,\n",
       "         1.8070e+00, -6.9972e-01,  2.9746e-01,  4.4737e+00,  2.6869e+00,\n",
       "         3.0212e+00,  8.2421e-01, -2.5050e-02,  1.6717e+00, -1.9168e+00,\n",
       "        -5.8327e-02, -1.0574e+00, -3.0999e+00,  3.9095e+00,  2.1009e+00,\n",
       "         5.9119e-01,  5.5733e-01,  2.9547e+00,  1.0841e+00, -3.5053e+00,\n",
       "        -5.7015e-01,  5.5908e-01,  1.6602e+00,  1.4821e+00,  3.2557e-01,\n",
       "         2.9879e+00,  1.8679e+00, -2.3491e+00,  4.7881e+00, -7.6344e+00,\n",
       "         1.5714e+00, -1.4656e+00, -1.4356e+00,  2.7338e+00, -3.9360e+00,\n",
       "        -5.6307e+00,  3.7394e+00, -6.5214e-01, -1.8036e+00,  2.0246e+00,\n",
       "         3.3150e+00,  1.2413e-01, -3.6524e-01, -1.3377e+00, -3.6453e+00,\n",
       "         4.9583e-01, -1.5435e+00, -3.9735e+00,  1.3627e+00,  1.4918e+00,\n",
       "         7.0674e-01, -4.4268e-01, -2.7360e+00,  3.9363e+00,  1.6205e+00,\n",
       "         1.1062e+00,  4.4475e-01,  3.2915e+00,  1.7187e+00, -2.0479e+00,\n",
       "         3.2590e+00,  6.4146e-01,  2.4258e+00,  1.0757e+00, -9.8107e-01,\n",
       "        -1.0013e+00,  1.1161e-01, -2.2753e+00, -2.2936e+00, -4.8648e+00,\n",
       "         1.1678e+00,  2.2442e+00, -5.8085e+00,  4.9467e-01, -4.7915e-01,\n",
       "        -1.5657e+00, -3.5452e+00, -4.6187e-01, -1.6781e+00,  1.9876e+00,\n",
       "         1.1937e+00,  6.3437e-01, -1.1289e+00,  1.3949e+00,  1.2534e+00,\n",
       "        -1.2882e+00,  7.8444e-01,  1.4245e-02, -2.0434e+00,  4.5580e+00,\n",
       "        -1.9623e+00, -6.2763e-01,  1.5380e+00,  5.3040e-01, -4.0692e+00,\n",
       "        -3.5452e+00,  1.3936e+00, -2.2420e+00,  8.2385e-01, -2.7391e+00,\n",
       "         1.1570e+00,  6.9187e+00,  1.1798e+00, -7.1172e-01, -8.1821e-01,\n",
       "        -1.3312e+00,  1.2027e+00,  2.9113e+00, -3.5176e-01, -1.5459e+00,\n",
       "        -2.9578e+00, -2.9971e+00,  2.2952e+00,  2.0967e+00,  3.8554e-01,\n",
       "        -9.1963e-02, -3.3757e-01, -3.0331e-01, -9.0219e-01, -8.8407e-01,\n",
       "         1.4099e+00, -1.1000e+00, -5.5946e-01,  2.7270e+00, -1.1351e+00,\n",
       "        -3.4229e+00,  8.5481e-01, -1.4283e+00, -2.3355e+00, -2.6046e+00,\n",
       "         3.1238e+00,  3.2590e+00,  1.6542e+00,  5.1748e-01, -5.3959e+00,\n",
       "         3.1737e+00,  2.4247e+00, -2.6147e+00, -4.3855e-01, -2.9611e+00,\n",
       "        -1.8359e+00, -1.5591e+00, -2.4580e+00, -1.6731e+00,  3.1173e+00,\n",
       "         2.9385e-02,  2.7460e-01, -1.4220e+00,  2.4812e+00,  1.6692e+00,\n",
       "         6.1330e-03,  1.6395e+00,  3.0102e+00, -7.2923e-01,  1.2154e+00,\n",
       "         1.5098e+00,  6.2281e-01,  1.3946e-01,  3.5543e+00, -2.5315e+00,\n",
       "         1.8225e+00, -2.1996e+00, -1.9431e-01,  6.1431e-01,  9.6891e-01,\n",
       "        -2.2008e-01, -1.0684e+00,  3.7471e-01, -1.2223e+00, -9.3747e-01,\n",
       "        -8.2960e-01,  2.9739e+00, -2.3159e+00, -3.3708e+00,  6.6436e-01,\n",
       "         1.9979e-01, -1.2504e+00,  1.2540e-01, -5.0948e-01,  4.5786e-01,\n",
       "         1.9975e-01, -1.8475e+00, -2.4195e+00, -3.3898e+00, -2.2660e-01,\n",
       "        -1.9724e+00,  8.3438e-01,  2.6732e+00, -2.0554e+00,  5.9141e-01,\n",
       "         3.1087e+00,  4.4605e+00,  2.6482e-01, -1.7331e+00,  1.3178e+00,\n",
       "         1.6483e+00, -1.8654e+00, -1.0064e-01,  4.0468e+00,  9.0628e-01,\n",
       "        -9.0628e-01,  3.0796e+00,  1.6066e+00, -9.5519e-01, -1.0820e+00,\n",
       "        -9.5716e-01, -1.2851e+00,  4.7522e+00, -1.0167e+00, -8.1386e-01,\n",
       "        -1.2278e+00,  2.9199e-01, -1.7143e-01, -2.1585e+00, -2.0144e+00,\n",
       "         9.2932e-01, -1.9020e-01,  1.0631e+00], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank1_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank1_vector = bank1_vector.detach().numpy()\n",
    "bank2_vector = bank2_vector.detach().numpy()\n",
    "bank3_vector = bank3_vector.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.35960817e+00, -2.98053288e+00, -1.54206288e+00,  7.06507444e-01,\n",
       "        2.00309658e+00,  6.31816864e-01, -2.90779924e+00,  1.63068748e+00,\n",
       "       -1.05810475e+00, -2.44669199e+00,  1.51981100e-01, -1.86486936e+00,\n",
       "       -8.66265416e-01,  1.55907416e+00, -4.40901184e+00,  1.99794710e-01,\n",
       "       -3.91201079e-01,  4.67799616e+00,  4.00633097e+00,  1.92805016e+00,\n",
       "       -2.68391442e+00, -4.57308561e-01,  2.95727468e+00,  2.06476378e+00,\n",
       "        7.15093851e-01,  5.05269110e-01,  2.97283244e+00,  2.07611656e+00,\n",
       "        3.02179068e-01,  5.94466925e-01,  3.27891254e+00,  1.39237666e+00,\n",
       "        1.76532102e+00,  3.65631282e-01,  4.89861846e-01, -5.83354831e-01,\n",
       "       -2.13272190e+00,  1.04740405e+00, -2.10780120e+00,  1.88239551e+00,\n",
       "       -5.26160479e-01, -1.44091487e+00, -1.20639503e-02,  1.69860947e+00,\n",
       "       -1.18508838e-01,  1.43692791e+00, -1.58004260e+00,  2.07835436e-01,\n",
       "       -4.06477118e+00, -2.74076313e-01, -1.26704466e+00,  2.74485707e+00,\n",
       "       -5.66275406e+00, -3.82622027e+00, -4.54449445e-01,  2.03629494e+00,\n",
       "       -3.36066484e+00, -3.47389340e+00,  4.67437804e-01, -1.83181167e+00,\n",
       "        2.76686239e+00,  1.11671185e+00,  3.07542920e+00, -4.98683786e+00,\n",
       "       -1.87892854e+00, -3.23852658e+00,  2.33783078e+00,  7.92549849e-01,\n",
       "       -1.70204771e+00,  2.16804218e+00,  7.55598724e-01,  6.98384142e+00,\n",
       "       -3.96148992e+00, -2.09889245e+00,  7.59477496e-01,  1.33489132e+00,\n",
       "       -1.22440219e+00,  1.37760484e+00,  8.40252995e-01,  5.68958372e-02,\n",
       "        5.20011485e-01, -3.12956429e+00, -1.04631793e+00,  8.54252279e-02,\n",
       "        1.79911482e+00,  2.18921423e-01,  1.87859905e+00,  5.05450666e-01,\n",
       "        9.63155687e-01,  1.05211818e+00, -2.41393399e+00,  2.74569941e+00,\n",
       "       -8.54676962e-01,  4.62167412e-01, -2.47786546e+00,  1.23514593e+00,\n",
       "       -7.34777451e-01,  3.42643380e-01, -1.83888841e+00, -1.21832192e-01,\n",
       "        1.34386194e+00, -7.45034218e+00, -2.81618118e-01,  2.03282237e+00,\n",
       "       -7.62483597e-01, -2.61432916e-01, -2.83154559e+00, -9.23236489e-01,\n",
       "       -2.21437979e+00, -1.40628427e-01,  3.65774369e+00, -1.82003307e+00,\n",
       "        3.00243527e-01, -3.37714005e+00,  1.41459250e+00, -2.07931471e+00,\n",
       "        2.76251745e+00, -4.99802494e+00, -3.44672918e-01, -4.33397102e+00,\n",
       "        3.03644776e-01,  3.28304291e+00,  1.70124888e+00,  2.01021504e+00,\n",
       "        1.37347078e+00,  1.24248242e+00,  6.72715545e-01,  1.99479055e+00,\n",
       "        4.42116529e-01,  4.65252548e-01,  2.33982944e+00,  6.57381654e-01,\n",
       "       -2.63763094e+00, -4.77609634e-01,  3.56332541e+00,  1.17650998e+00,\n",
       "       -1.38007569e+00, -1.94869757e+00, -3.30135465e+00, -1.49794197e+00,\n",
       "        7.56535769e-01,  1.53158247e+00, -1.16492987e+00,  7.39455795e+00,\n",
       "        1.25963926e+00,  1.38602626e+00, -2.12257528e+00, -1.80463552e+00,\n",
       "       -1.07787704e+00,  2.53702784e+00, -1.30950141e+00,  2.01663542e+00,\n",
       "       -1.63547069e-01, -5.92484236e-01,  3.60534430e+00, -9.87099528e-01,\n",
       "       -4.66663361e+00,  1.22678208e+00, -2.46888185e+00, -6.22462630e-01,\n",
       "        1.53183126e+00, -7.80709535e-02,  2.17274094e+00,  1.34728980e+00,\n",
       "       -5.78887367e+00, -4.62906742e+00, -2.36536205e-01,  1.33857369e+00,\n",
       "       -2.25045252e+00, -1.81933188e+00, -2.19766378e-01, -1.11539733e+00,\n",
       "        1.30608320e+00,  8.00547361e-01,  3.73043060e+00,  2.23176932e+00,\n",
       "       -1.02648544e+00, -2.77858424e+00, -6.04407936e-02, -2.61875153e+00,\n",
       "       -4.59284544e-01,  3.29027939e+00,  2.57588434e+00,  4.17139816e+00,\n",
       "        2.77097321e+00, -1.47712374e+00,  9.15765572e+00,  1.49191856e+00,\n",
       "        2.21328759e+00, -1.48205861e-01, -1.06888175e+00, -3.92129946e+00,\n",
       "       -2.64814526e-01,  4.03774768e-01, -2.67901874e+00, -3.25615615e-01,\n",
       "       -1.94450068e+00, -2.07957911e+00, -2.55338287e+00, -4.23997462e-01,\n",
       "       -2.52424955e+00, -9.24976051e-01, -2.57258892e+00, -1.35025835e+00,\n",
       "       -3.21844006e+00,  1.32747114e+00, -1.37052441e+00,  1.79354358e+00,\n",
       "        1.62987995e+00, -4.06234562e-01, -4.55986410e-01, -1.06076384e+00,\n",
       "        7.02371120e-01,  2.66776872e+00, -7.95199335e-01,  1.63788092e+00,\n",
       "       -1.36656559e+00, -4.57933760e+00, -7.76154459e-01, -1.31902111e+00,\n",
       "       -1.33727419e+00, -9.48733389e-01,  1.25399518e+00,  3.33155662e-01,\n",
       "       -2.69639730e+00,  2.57589316e+00,  3.57635188e+00, -1.06565893e+00,\n",
       "       -2.85291409e+00,  6.12420917e-01, -6.81671977e-01,  4.13369799e+00,\n",
       "        2.61777115e+00,  5.03383577e-02,  2.13056660e+00, -1.70185804e+00,\n",
       "       -6.25876093e+00,  1.39035511e+00,  1.02627468e+00,  2.72400212e+00,\n",
       "       -1.70207214e+00,  2.46422291e-01,  2.49052238e+00, -1.08295292e-01,\n",
       "        2.08896324e-01,  2.58477092e+00, -3.33723426e-01, -2.64095592e+00,\n",
       "       -3.88241339e+00,  4.68168199e-01, -1.29403114e+00, -1.03590453e+00,\n",
       "       -3.19901705e+00, -9.10030484e-01,  4.86277151e+00, -5.31920493e-01,\n",
       "       -6.94040298e-01, -3.87080073e-01, -2.46785378e+00,  1.14176667e+00,\n",
       "        2.70122814e+00,  2.04981852e+00, -2.42343283e+00,  9.85824168e-01,\n",
       "       -8.97721410e-01, -2.32259226e+00,  1.28676903e+00,  7.52717078e-01,\n",
       "        6.84866965e-01,  2.38214540e+00,  6.64937496e-01,  2.63695574e+00,\n",
       "       -1.72903180e-01,  4.43381071e+00,  3.54703951e+00, -2.33682251e+00,\n",
       "       -1.77922165e+00, -1.35108244e+00,  3.20280051e+00,  2.31250596e+00,\n",
       "        1.59854889e-02,  1.77982700e+00,  7.01709509e-01,  2.85609543e-01,\n",
       "        4.56307828e-02, -1.03847277e+00, -4.37443972e-01,  5.99897861e+00,\n",
       "       -1.01188743e+00, -2.03110218e+00,  5.82613707e-01,  7.26915970e-02,\n",
       "       -1.62928200e+00,  8.29983413e-01,  6.34198785e-01,  8.03585172e-01,\n",
       "        1.38047361e+00,  5.12956095e+00,  2.90977120e+00, -4.53440785e-01,\n",
       "        2.98783803e+00,  3.70412827e-01, -9.34061468e-01, -4.29342842e+00,\n",
       "       -1.45594215e+00,  1.06365800e+00, -1.10914990e-01, -3.52073133e-01,\n",
       "       -2.16034203e+01,  5.84332561e+00, -4.01795506e-01, -2.26246047e+00,\n",
       "        1.16320777e+00, -9.18178082e-01,  1.08077395e+00, -1.77477908e+00,\n",
       "       -5.63172197e+00,  7.62289703e-01, -4.37209225e+00, -2.34590411e+00,\n",
       "        1.86517620e+00, -7.37911284e-01, -3.41636330e-01, -4.16405487e+00,\n",
       "        3.79883647e-02,  1.65699363e+00, -2.89048254e-01, -1.27503681e+00,\n",
       "       -1.45242131e+00, -2.65711379e+00, -4.08691943e-01,  1.97331905e+00,\n",
       "        2.49950266e+00,  2.37885714e+00, -2.19283628e+00, -1.02055073e+00,\n",
       "       -2.98197269e-01,  1.63518941e+00, -1.70111680e+00,  1.98058009e-01,\n",
       "       -1.21861055e-01,  1.13330698e+00, -1.17206883e+00, -3.10141945e+00,\n",
       "       -2.57970285e+00, -2.04010773e+00,  2.03680229e+00, -3.56840491e+00,\n",
       "        6.74297750e-01,  3.25021243e+00, -2.66172719e+00,  2.20802569e+00,\n",
       "        3.26120806e+00, -1.52001262e+00, -3.57511640e-01,  1.95358050e+00,\n",
       "       -6.91266060e-01,  2.82043171e+00,  5.88026285e-01, -2.05465078e+00,\n",
       "        1.16132033e+00, -7.21801043e-01,  1.28962207e+00,  2.01992702e+00,\n",
       "       -9.27550614e-01,  1.84996426e+00,  3.51469755e+00, -1.17487931e+00,\n",
       "       -1.39672506e+00,  2.07798123e+00,  6.34122610e-01,  8.59730124e-01,\n",
       "       -1.34719312e+00,  2.88664430e-01, -6.08762646e+00, -2.46520853e+00,\n",
       "       -3.14039409e-01, -2.63904738e+00, -2.41320539e+00,  9.56782162e-01,\n",
       "        6.52985722e-02, -1.05736628e+01, -8.27195823e-01, -1.41969800e+00,\n",
       "        3.95983815e+00, -3.71420050e+00, -4.80801868e+00,  9.75403905e-01,\n",
       "        3.11445808e+00,  2.00718731e-01,  2.05827880e+00, -4.33251619e-01,\n",
       "        1.61659217e+00,  1.17398846e+00, -1.73906159e+00,  1.88226426e+00,\n",
       "       -5.70513582e+00,  3.14880848e-01,  2.41121793e+00,  6.19218254e+00,\n",
       "        2.14729857e+00, -2.87073016e+00,  5.08110166e-01,  2.48933983e+00,\n",
       "       -1.25328088e+00, -1.17522347e+00,  1.50371039e+00,  4.97656435e-01,\n",
       "       -6.95943907e-02,  4.07157516e+00,  2.78406000e+00, -3.08396637e-01,\n",
       "       -2.62973142e+00, -2.73444700e+00,  5.66433549e-01, -1.03925955e+00,\n",
       "       -4.33391809e+00,  5.87707281e+00,  2.25125480e+00, -2.78065205e+00,\n",
       "        3.97324562e+00, -2.54492855e+00, -1.98300809e-01,  1.16484046e+00,\n",
       "       -4.14034188e-01, -3.37409925e+00, -1.93793103e-01, -2.01515317e+00,\n",
       "       -4.56725264e+00,  5.28545916e-01, -1.63005567e+00, -6.22604084e+00,\n",
       "       -5.01335859e-01,  1.68446374e+00,  1.39162827e+00, -1.24533510e+00,\n",
       "       -1.52294850e+00, -8.94635379e-01,  3.09942245e+00, -7.46836424e-01,\n",
       "       -2.78749704e+00,  4.45153803e-01,  1.07106328e+00, -3.49408656e-01,\n",
       "        3.80733776e+00, -6.61009312e-01, -2.02553058e+00,  2.40203962e-02,\n",
       "        1.31796193e+00,  7.80625880e-01,  8.57545614e-01, -1.24423265e+00,\n",
       "       -1.05592653e-01, -3.36620998e+00, -1.42594123e+00,  1.36610031e+00,\n",
       "        8.77478540e-01, -1.07114697e+00, -1.28272748e+00, -6.54980421e-01,\n",
       "        2.38863015e+00, -9.38175857e-01,  2.42481518e+00,  4.38931942e+00,\n",
       "        5.26189804e+00, -1.45820141e+00, -3.22000337e+00, -1.88623405e+00,\n",
       "        2.13055134e+00, -2.58285284e+00,  4.50317669e+00,  1.53421497e+00,\n",
       "       -8.15723419e+00, -4.56696540e-01, -4.98010063e+00, -2.10885453e+00,\n",
       "        5.53418159e-01,  1.27043748e+00,  1.27267611e+00, -2.06869936e+00,\n",
       "       -7.38990366e-01,  2.56924772e+00, -1.34030449e+00,  9.59775090e-01,\n",
       "        6.77857161e-01, -3.39887691e+00, -8.59050304e-02,  1.10101581e+00,\n",
       "       -9.02763486e-01,  3.95995927e+00,  2.23665833e+00, -2.11766720e+00,\n",
       "        8.37030232e-01,  2.85812473e+00,  2.13563704e+00,  3.68496943e+00,\n",
       "       -1.18879890e+00,  1.34280622e-01, -2.61064577e+00,  1.35095191e+00,\n",
       "       -6.33587897e-01,  9.07543540e-01, -3.11445093e+00, -3.69282651e+00,\n",
       "       -7.12531924e-01,  1.27372348e+00, -4.14643645e-01,  3.79166663e-01,\n",
       "       -4.16440868e+00,  1.59851646e+00,  8.28232244e-03, -2.39628220e+00,\n",
       "        5.15241563e-01,  1.42191350e-02,  1.63853383e+00,  9.90855813e-01,\n",
       "       -2.26349282e+00,  3.36629009e+00, -4.53697300e+00, -1.25800222e-01,\n",
       "       -4.60923767e+00, -1.98354030e+00,  1.34837711e+00, -2.42087674e+00,\n",
       "        1.01941788e+00, -2.27275044e-01, -1.95109200e+00, -4.62375507e-02,\n",
       "        2.27486348e+00,  9.66746747e-01, -9.66557086e-01, -1.36344075e+00,\n",
       "       -2.87966633e+00,  6.82333350e-01,  1.47124195e+00,  1.45533526e+00,\n",
       "       -1.01508951e+00,  1.80987692e+00,  2.26176828e-02, -5.47281599e+00,\n",
       "        7.53778875e-01, -6.08592391e-01, -1.77374303e+00, -4.67718929e-01,\n",
       "       -8.83141458e-01, -7.17372847e+00,  5.82953274e-01, -8.53997648e-01,\n",
       "        1.55931115e-02,  5.43450952e-01,  1.80699956e+00, -6.99716151e-01,\n",
       "        2.97456920e-01,  4.47371864e+00,  2.68686724e+00,  3.02122498e+00,\n",
       "        8.24212015e-01, -2.50502825e-02,  1.67172813e+00, -1.91675532e+00,\n",
       "       -5.83271086e-02, -1.05741906e+00, -3.09985828e+00,  3.90953183e+00,\n",
       "        2.10091162e+00,  5.91188073e-01,  5.57326317e-01,  2.95472765e+00,\n",
       "        1.08407998e+00, -3.50529242e+00, -5.70145786e-01,  5.59078932e-01,\n",
       "        1.66020226e+00,  1.48210490e+00,  3.25567663e-01,  2.98790956e+00,\n",
       "        1.86794066e+00, -2.34906340e+00,  4.78806257e+00, -7.63439512e+00,\n",
       "        1.57141852e+00, -1.46557879e+00, -1.43561411e+00,  2.73380184e+00,\n",
       "       -3.93596888e+00, -5.63073969e+00,  3.73943639e+00, -6.52143717e-01,\n",
       "       -1.80364799e+00,  2.02455902e+00,  3.31502247e+00,  1.24129102e-01,\n",
       "       -3.65236759e-01, -1.33769429e+00, -3.64532685e+00,  4.95832682e-01,\n",
       "       -1.54347682e+00, -3.97350335e+00,  1.36269259e+00,  1.49178517e+00,\n",
       "        7.06736088e-01, -4.42676723e-01, -2.73595905e+00,  3.93627071e+00,\n",
       "        1.62052369e+00,  1.10620284e+00,  4.44753766e-01,  3.29153347e+00,\n",
       "        1.71865654e+00, -2.04791689e+00,  3.25899029e+00,  6.41458988e-01,\n",
       "        2.42581320e+00,  1.07572961e+00, -9.81070220e-01, -1.00133812e+00,\n",
       "        1.11613885e-01, -2.27527189e+00, -2.29362392e+00, -4.86477184e+00,\n",
       "        1.16783094e+00,  2.24416542e+00, -5.80853319e+00,  4.94668067e-01,\n",
       "       -4.79148030e-01, -1.56568861e+00, -3.54516268e+00, -4.61874038e-01,\n",
       "       -1.67808080e+00,  1.98755479e+00,  1.19365203e+00,  6.34371400e-01,\n",
       "       -1.12894094e+00,  1.39493179e+00,  1.25338674e+00, -1.28818893e+00,\n",
       "        7.84437418e-01,  1.42452419e-02, -2.04342747e+00,  4.55800104e+00,\n",
       "       -1.96231890e+00, -6.27629697e-01,  1.53803897e+00,  5.30397534e-01,\n",
       "       -4.06917858e+00, -3.54524565e+00,  1.39359295e+00, -2.24204493e+00,\n",
       "        8.23848128e-01, -2.73908257e+00,  1.15702546e+00,  6.91866159e+00,\n",
       "        1.17984629e+00, -7.11715817e-01, -8.18214238e-01, -1.33116460e+00,\n",
       "        1.20268905e+00,  2.91127634e+00, -3.51763725e-01, -1.54586768e+00,\n",
       "       -2.95775080e+00, -2.99707937e+00,  2.29520583e+00,  2.09670281e+00,\n",
       "        3.85536849e-01, -9.19633359e-02, -3.37573767e-01, -3.03309619e-01,\n",
       "       -9.02192056e-01, -8.84072065e-01,  1.40985811e+00, -1.10004425e+00,\n",
       "       -5.59455276e-01,  2.72703886e+00, -1.13508582e+00, -3.42286086e+00,\n",
       "        8.54806900e-01, -1.42834556e+00, -2.33545041e+00, -2.60455489e+00,\n",
       "        3.12384963e+00,  3.25903940e+00,  1.65417194e+00,  5.17484605e-01,\n",
       "       -5.39589405e+00,  3.17370272e+00,  2.42466760e+00, -2.61466312e+00,\n",
       "       -4.38554287e-01, -2.96108365e+00, -1.83589149e+00, -1.55906439e+00,\n",
       "       -2.45798802e+00, -1.67305326e+00,  3.11733961e+00,  2.93850899e-02,\n",
       "        2.74603009e-01, -1.42203522e+00,  2.48123837e+00,  1.66922069e+00,\n",
       "        6.13303483e-03,  1.63951993e+00,  3.01016378e+00, -7.29228735e-01,\n",
       "        1.21535373e+00,  1.50976062e+00,  6.22810245e-01,  1.39459178e-01,\n",
       "        3.55427885e+00, -2.53150535e+00,  1.82245529e+00, -2.19963217e+00,\n",
       "       -1.94307745e-01,  6.14314914e-01,  9.68907595e-01, -2.20080659e-01,\n",
       "       -1.06837058e+00,  3.74706209e-01, -1.22225571e+00, -9.37465489e-01,\n",
       "       -8.29596579e-01,  2.97394872e+00, -2.31588244e+00, -3.37076616e+00,\n",
       "        6.64363921e-01,  1.99791506e-01, -1.25036168e+00,  1.25396937e-01,\n",
       "       -5.09475231e-01,  4.57860351e-01,  1.99753404e-01, -1.84753358e+00,\n",
       "       -2.41952562e+00, -3.38980293e+00, -2.26599902e-01, -1.97236419e+00,\n",
       "        8.34375620e-01,  2.67316890e+00, -2.05539846e+00,  5.91408372e-01,\n",
       "        3.10869503e+00,  4.46046066e+00,  2.64822453e-01, -1.73309231e+00,\n",
       "        1.31781650e+00,  1.64826918e+00, -1.86544847e+00, -1.00635216e-01,\n",
       "        4.04677200e+00,  9.06283021e-01, -9.06275630e-01,  3.07957983e+00,\n",
       "        1.60662293e+00, -9.55187023e-01, -1.08197188e+00, -9.57162738e-01,\n",
       "       -1.28512728e+00,  4.75220585e+00, -1.01669991e+00, -8.13862205e-01,\n",
       "       -1.22782469e+00,  2.91986376e-01, -1.71432436e-01, -2.15848255e+00,\n",
       "       -2.01441884e+00,  9.29321289e-01, -1.90195397e-01,  1.06311262e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank1_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9386392"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(bank1_vector, bank2_vector)/(np.linalg.norm(bank1_vector)*np.linalg.norm(bank2_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69579333"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(bank1_vector, bank3_vector)/(np.linalg.norm(bank1_vector)*np.linalg.norm(bank3_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69323605"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(bank3_vector, bank2_vector)/(np.linalg.norm(bank3_vector)*np.linalg.norm(bank2_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
