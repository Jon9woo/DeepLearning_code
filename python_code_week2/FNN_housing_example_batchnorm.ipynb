{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "(train_data, train_targets), (test_data, test_targets) =keras.datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "# Regularizer 더하기\n",
    "# https://keras.io/api/layers/regularizers/\n",
    "model.add(layers.Dense(32, activation = 'relu', kernel_regularizer=regularizers.l2(0.1), input_shape=(train_data.shape[1],)))\n",
    "# Dropout 적용하기\n",
    "model.add(layers.Dropout(0.50))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(64, activation = 'relu', kernel_regularizer=regularizers.l1(0.2)))\n",
    "model.add(layers.Dropout(0.30))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                448       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,009\n",
      "Trainable params: 2,817\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "21/21 [==============================] - 0s 17ms/step - loss: 159.8709 - val_loss: 287.0126\n",
      "Epoch 2/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 89.9253 - val_loss: 85.5331\n",
      "Epoch 3/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 67.4184 - val_loss: 58.0129\n",
      "Epoch 4/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 75.9447 - val_loss: 47.4945\n",
      "Epoch 5/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 62.4939 - val_loss: 63.5690\n",
      "Epoch 6/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 68.5655 - val_loss: 63.0904\n",
      "Epoch 7/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 92.4932 - val_loss: 78.8732\n",
      "Epoch 8/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 76.6490 - val_loss: 30.4677\n",
      "Epoch 9/80\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 70.2418 - val_loss: 61.2577\n",
      "Epoch 10/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 78.9583 - val_loss: 41.6800\n",
      "Epoch 11/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 56.1398 - val_loss: 66.1564\n",
      "Epoch 12/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 57.8825 - val_loss: 42.4729\n",
      "Epoch 13/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 55.9600 - val_loss: 30.4988\n",
      "Epoch 14/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 89.4707 - val_loss: 61.3210\n",
      "Epoch 15/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 65.7927 - val_loss: 47.9129\n",
      "Epoch 16/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 65.6166 - val_loss: 45.0043\n",
      "Epoch 17/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 55.8420 - val_loss: 72.3631\n",
      "Epoch 18/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 60.0925 - val_loss: 32.1671\n",
      "Epoch 19/80\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 64.1756 - val_loss: 41.0166\n",
      "Epoch 20/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 72.7421 - val_loss: 46.4297\n",
      "Epoch 21/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 53.0805 - val_loss: 39.1737\n",
      "Epoch 22/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 59.8480 - val_loss: 41.2638\n",
      "Epoch 23/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 60.9554 - val_loss: 53.3914\n",
      "Epoch 24/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 64.5683 - val_loss: 36.6774\n",
      "Epoch 25/80\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 53.4438 - val_loss: 29.0910\n",
      "Epoch 26/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 49.2839 - val_loss: 40.5511\n",
      "Epoch 27/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 59.9709 - val_loss: 51.0075\n",
      "Epoch 28/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 57.7516 - val_loss: 32.1850\n",
      "Epoch 29/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 63.8783 - val_loss: 53.5293\n",
      "Epoch 30/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 63.6460 - val_loss: 41.9076\n",
      "Epoch 31/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 58.1538 - val_loss: 37.8697\n",
      "Epoch 32/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 54.1282 - val_loss: 42.2071\n",
      "Epoch 33/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 55.8408 - val_loss: 37.6298\n",
      "Epoch 34/80\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 59.5059 - val_loss: 46.6342\n",
      "Epoch 35/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 70.1152 - val_loss: 42.4292\n",
      "Epoch 36/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 58.3024 - val_loss: 35.9530\n",
      "Epoch 37/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 59.8457 - val_loss: 43.8533\n",
      "Epoch 38/80\n",
      "21/21 [==============================] - ETA: 0s - loss: 37.52 - 0s 3ms/step - loss: 58.9372 - val_loss: 43.4963\n",
      "Epoch 39/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 73.9429 - val_loss: 46.2907\n",
      "Epoch 40/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 65.2281 - val_loss: 62.1596\n",
      "Epoch 41/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 70.7265 - val_loss: 45.5798\n",
      "Epoch 42/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 62.9058 - val_loss: 43.1804\n",
      "Epoch 43/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 57.3296 - val_loss: 43.6636\n",
      "Epoch 44/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 61.0689 - val_loss: 46.9383\n",
      "Epoch 45/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 58.3152 - val_loss: 68.1572\n",
      "Epoch 46/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 66.2998 - val_loss: 45.5323\n",
      "Epoch 47/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 63.8442 - val_loss: 42.8889\n",
      "Epoch 48/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 58.5408 - val_loss: 53.7441\n",
      "Epoch 49/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 63.5111 - val_loss: 40.4624\n",
      "Epoch 50/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 62.3797 - val_loss: 51.3392\n",
      "Epoch 51/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 58.0352 - val_loss: 37.5285\n",
      "Epoch 52/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 54.2254 - val_loss: 44.0051\n",
      "Epoch 53/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 57.2678 - val_loss: 35.7083\n",
      "Epoch 54/80\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 59.7953 - val_loss: 52.2862\n",
      "Epoch 55/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 60.7230 - val_loss: 43.8119\n",
      "Epoch 56/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 53.3178 - val_loss: 42.7968\n",
      "Epoch 57/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 57.5388 - val_loss: 38.7418\n",
      "Epoch 58/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 55.9876 - val_loss: 47.9478\n",
      "Epoch 59/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 58.7282 - val_loss: 46.5414\n",
      "Epoch 60/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 52.3754 - val_loss: 43.0167\n",
      "Epoch 61/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 66.6183 - val_loss: 61.2182\n",
      "Epoch 62/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 58.5030 - val_loss: 58.8371\n",
      "Epoch 63/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 60.8047 - val_loss: 47.0043\n",
      "Epoch 64/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 66.1327 - val_loss: 45.3216\n",
      "Epoch 65/80\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 65.1697 - val_loss: 58.2324\n",
      "Epoch 66/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 71.8941 - val_loss: 110.4432\n",
      "Epoch 67/80\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 76.2737 - val_loss: 52.9304\n",
      "Epoch 68/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 75.2356 - val_loss: 60.3328\n",
      "Epoch 69/80\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 94.2636 - val_loss: 84.0682\n",
      "Epoch 70/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 73.0930 - val_loss: 45.4841\n",
      "Epoch 71/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 69.9935 - val_loss: 54.2818\n",
      "Epoch 72/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 69.1145 - val_loss: 57.3905\n",
      "Epoch 73/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 65.9781 - val_loss: 54.4667\n",
      "Epoch 74/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 68.3440 - val_loss: 55.3698\n",
      "Epoch 75/80\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 89.9824 - val_loss: 57.8920\n",
      "Epoch 76/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 85.0697 - val_loss: 52.6346\n",
      "Epoch 77/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 59.0289 - val_loss: 59.1777\n",
      "Epoch 78/80\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 75.7690 - val_loss: 75.9345\n",
      "Epoch 79/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 72.2733 - val_loss: 65.7383\n",
      "Epoch 80/80\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 63.7166 - val_loss: 47.7867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28559d75f28>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_targets, epochs=80, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1243.1052517483458"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(test_targets, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
